{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bffeb280",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import ast\n",
    "import validators\n",
    "import string\n",
    "import demoji\n",
    "import pyLDAvis.sklearn\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb592696",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def DFToDocument(df):\n",
    "    document = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for i, row in df.iterrows():\n",
    "        sentence = \"\"\n",
    "        tokenized_text = ast.literal_eval(row['tokenized_text'])\n",
    "        pos_tags = pos_tag(tokenized_text)\n",
    "        tonkenized_text = [token for token, pos in pos_tags if pos not in ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'VB']]\n",
    "        for token in tokenized_text:\n",
    "            if token not in stop_words and not validators.url(token) and token not in string.punctuation and not re.search(r'(chat[-\\s])?gpt|\\d+', token.lower()):\n",
    "                sentence += lemmatizer.lemmatize(token) + ' '\n",
    "        sentence = demoji.replace(string = sentence, repl = \"\")\n",
    "        document.append(sentence[:-1])\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ac0c03f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35371, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('translated_tokenized/translated_dataframe.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215285c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "document = DFToDocument(df)\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9993dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CountVectorizer converts the text documents to a matrix of token counts\n",
    "# vectorizer = CountVectorizer()\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "docs_vecs = vectorizer.fit_transform(document)\n",
    "\n",
    "# get_feature_names saves all the words. This allows us to see the words in each topic later\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65a9973",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "docs_lda = LatentDirichletAllocation(n_components=10,               # Number of topics\n",
    "                                      learning_method='online',   \n",
    "                                      n_jobs = -1, random_state=1)              # use all available CPU\n",
    "lda_output = docs_lda.fit_transform(docs_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a283d090",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# write a function to display the 10 topics and for each topic we choose the top most frequent words. \n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "no_top_words = 15\n",
    "display_topics(docs_lda, feature_names, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355dc796",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df = pd.DataFrame({\"text\": document, \"topic\": lda_output.argmax(axis=1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316c6dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in range(docs_lda.n_components):\n",
    "    print(f\"Topic {topic}:\")\n",
    "    print(lda_df[lda_df['topic'] == topic].sort_values(by='topic', ascending=False).head(50))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06258c02",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ddeeb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Set up the environment to display the graphical outputs\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5507e491",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate the visuals\n",
    "visual = pyLDAvis.sklearn.prepare(docs_lda, docs_vecs, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934a0342",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91a160d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
